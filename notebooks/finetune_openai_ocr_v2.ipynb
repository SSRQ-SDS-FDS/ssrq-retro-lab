{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zz/66rgnps56_7bx2sjt1f7rsh80000gn/T/ipykernel_54194/3814869860.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ssrq_retro_lab.config import PROJECT_ROOT, ZG_DATA_ROOT\n",
    "from ssrq_retro_lab.pipeline.templates.utils import render_template\n",
    "from ssrq_retro_lab.repository.writer import JSONLWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_training_df = pd.read_pickle(\n",
    "    PROJECT_ROOT / \"notebooks\" / \"pkl_cache\" / \"ocr_line_based_training.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_training_df = pd.read_pickle(\n",
    "    PROJECT_ROOT / \"notebooks\" / \"pkl_cache\" / \"ocr_line_based_training.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(ocr_training_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, 124)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ssrq_retro_lab.train.messages import SYSTEM_ROLE_V2\n",
    "from ssrq_retro_lab.pipeline.components.ocr_corrector import CorrectedOCRText\n",
    "\n",
    "schema = json.dumps(CorrectedOCRText.model_json_schema(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_messages(batches: list[pd.DataFrame]) -> list[dict[str, list[dict[str, str]]]]:\n",
    "    training_messages = []\n",
    "    for batch in batches:\n",
    "        messages = {\"messages\": []}\n",
    "\n",
    "        user_input = render_template(\n",
    "            template_name=\"openai_ocr_training_user_v2.jinja2\",\n",
    "            schema=schema,\n",
    "            text_input=json.dumps(batch[\"source\"].tolist(), ensure_ascii=False),\n",
    "        )\n",
    "        assistant_output = (\n",
    "            \"```json\\n\"\n",
    "            + CorrectedOCRText(text=batch[\"target\"].tolist()).model_dump_json(indent=2)\n",
    "            + \"\\n```\"\n",
    "        )\n",
    "\n",
    "        messages[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_ROLE_V2,\n",
    "            }\n",
    "        )\n",
    "        messages[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        )\n",
    "        messages[\"messages\"].append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_output,\n",
    "            }\n",
    "        )\n",
    "        training_messages.append(messages)\n",
    "    return training_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_batches: list[pd.DataFrame] = [\n",
    "    batch for _, batch in train.groupby(np.arange(len(train)) // 5)\n",
    "]\n",
    "\n",
    "validation_batches: list[pd.DataFrame] = [\n",
    "    batch for _, batch in test.groupby(np.arange(len(test)) // 5)\n",
    "]\n",
    "\n",
    "training_messages: list[dict[str, list[dict[str, str]]]] = create_training_messages(\n",
    "    training_batches\n",
    ")\n",
    "\n",
    "validation_messages: list[dict[str, list[dict[str, str]]]] = create_training_messages(\n",
    "    validation_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONLWriter(ZG_DATA_ROOT / \"training_data\" / \"gpt_ocr\" / \"train.jsonl\").write(\n",
    "    content=training_messages\n",
    ")\n",
    "\n",
    "JSONLWriter(ZG_DATA_ROOT / \"training_data\" / \"gpt_ocr\" / \"test.jsonl\").write(\n",
    "    content=training_messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 409, 606\n",
      "mean / median: 509.42424242424244, 509.0\n",
      "p5 / p95: 471.0, 555.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 76, 172\n",
      "mean / median: 126.52525252525253, 127.0\n",
      "p5 / p95: 107.6, 150.0\n",
      "\n",
      "0 examples may be over the 16385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from ssrq_retro_lab.train.stats import (\n",
    "    num_assistant_tokens_from_messages,\n",
    "    num_tokens_from_messages,\n",
    "    print_distribution,\n",
    ")\n",
    "\n",
    "TOKEN_LIMIT = 16385\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in training_messages:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > TOKEN_LIMIT for l in convo_lens)\n",
    "print(\n",
    "    f\"\\n{n_too_long} examples may be over the {TOKEN_LIMIT} token limit, they will be truncated during fine-tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~50433 tokens that will be charged for during training\n",
      "By default, you'll train for 25 epochs on this dataset\n",
      "By default, you'll be charged for ~1260825 tokens\n",
      "Estimated cost for fine-tuning: approximately $10.09\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 75\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(messages)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(TOKEN_LIMIT, length) for length in convo_lens)\n",
    "print(\n",
    "    f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\"\n",
    ")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(\n",
    "    f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\"\n",
    ")\n",
    "\n",
    "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
    "estimated_cost = (\n",
    "    (n_epochs * n_billing_tokens_in_dataset) / 100000\n",
    ") * cost_per_100k_tokens\n",
    "print(\n",
    "    f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\"\n",
    ")  # I added this for actual cost based on current pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrq_retro_lab.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_id = train.upload_training_file_to_openai(\n",
    "    (ZG_DATA_ROOT / \"training_data\" / \"gpt_ocr\" / \"train.jsonl\")\n",
    ")\n",
    "\n",
    "test_file_id = train.upload_training_file_to_openai(\n",
    "    (ZG_DATA_ROOT / \"training_data\" / \"gpt_ocr\" / \"test.jsonl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "job_response = openai.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=test_file_id,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    suffix=\"ssrq-ocr-cor\",\n",
    "    hyperparameters={\"n_epochs\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ft:gpt-3.5-turbo-1106:personal:ssrq-ocr-cor:8tgnqalq'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model_id = model_id = train.get_finetuned_model_id(\n",
    "    job_response.id\n",
    ")\n",
    "\n",
    "finetuned_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle(\"./pkl_cache/openai_finetune_ocr_validation_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
