{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrq_retro_lab.config import PROJECT_ROOT\n",
    "from ssrq_retro_lab.repository import reader\n",
    "from ssrq_retro_lab.repository import writer\n",
    "from ssrq_retro_lab.train import data, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "txt_pdf_conversion_table = json.loads(\n",
    "    reader.TextReader((PROJECT_ROOT / \"data/ZG/txt_to_pdf.json\")).read()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitz_new import Document\n",
    "\n",
    "def get_page_text_from_pdf(pdf: Document, page: int) -> str:\n",
    "    return pdf.load_page(page).get_textpage().extractText(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = [pdf for pdf in (PROJECT_ROOT / \"data/ZG/pdf\").glob(\"*.pdf\")]\n",
    "master_transcriptions = [txt for txt in (PROJECT_ROOT / \"data/ZG/master\").glob(\"*[0-9].txt\")]\n",
    "\n",
    "training_data: list[data.OpenAIDataset] = []\n",
    "ocr_validation_data: list[data.OpenAIDataset] = []\n",
    "splitting_validation_data: list[data.OpenAIDataset] = []\n",
    "classification_validation_data: list[data.OpenAIDataset] = []\n",
    "\n",
    "validation_indicies = [3, 5, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will create different types of training data used for fine-tuning a custom GPT-model, which should act as a research assistant in the process (retro)digitizing the Collection of Swiss Law Sources. We will create training data for the following tasks:\n",
    "\n",
    "1. OCR-Correction\n",
    "2. Text segmentation\n",
    "3. Text classification\n",
    "\n",
    "The training data will be saved into the file `openai_training_data.jsonl`. For each type a ~10% split will be created as validation data and saved into seperate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for volume in volumes:\n",
    "    doc = reader.PDFReader(volume).read()\n",
    "    volume_name = volume.name.removesuffix(\".pdf\").replace(\".\", \"_\")\n",
    "    transcriptions = [\n",
    "        transcription\n",
    "        for transcription in master_transcriptions\n",
    "        if transcription.name.startswith(volume_name)\n",
    "    ]\n",
    "\n",
    "    for index, transcription in enumerate(transcriptions):\n",
    "        page_number = int(\n",
    "            txt_pdf_conversion_table[volume_name][\n",
    "                transcription.name.removesuffix(\".txt\")\n",
    "            ]\n",
    "        )\n",
    "        page_text = get_page_text_from_pdf(\n",
    "            doc,\n",
    "            page_number,\n",
    "        )\n",
    "        master_text = reader.TextReader(transcription).read()\n",
    "        training_set = data.create_openai_dataset(\n",
    "            system_role=messages.SYSTEM_ROLE,\n",
    "            user_template=messages.USER_OCR_CORRECTION,\n",
    "            user_text=page_text,\n",
    "            assistant_template=messages.ASSISTANT_OCR_CORRECTION,\n",
    "            assistant_text=master_text,\n",
    "        )\n",
    "\n",
    "        if index in validation_indicies:\n",
    "            ocr_validation_data.append(training_set)\n",
    "        else:\n",
    "            training_data.append(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for volume in volumes:\n",
    "    doc = reader.PDFReader(volume).read()\n",
    "    volume_name = volume.name.removesuffix(\".pdf\").replace(\".\", \"_\")\n",
    "    transcriptions = [\n",
    "        transcription\n",
    "        for transcription in master_transcriptions\n",
    "        if transcription.name.startswith(volume_name)\n",
    "    ]\n",
    "\n",
    "    for index, transcription in enumerate(transcriptions):\n",
    "        page_number = int(\n",
    "            txt_pdf_conversion_table[volume_name][\n",
    "                transcription.name.removesuffix(\".txt\")\n",
    "            ]\n",
    "        )\n",
    "        page_text = get_page_text_from_pdf(\n",
    "            doc,\n",
    "            page_number,\n",
    "        )\n",
    "        master_text = reader.TextReader(Path(str(transcription.absolute()).replace(\".txt\", \"_splitted.txt\"))).read()\n",
    "        training_set = data.create_openai_dataset(\n",
    "            system_role=messages.SYSTEM_ROLE,\n",
    "            user_template=messages.USER_TEXT_SPLITTING,\n",
    "            user_text=page_text,\n",
    "            assistant_template=messages.ASSISTANT_TEXT_SPLITTING,\n",
    "            assistant_text=master_text,\n",
    "        )\n",
    "\n",
    "        if index in validation_indicies:\n",
    "            splitting_validation_data.append(training_set)\n",
    "        else:\n",
    "            training_data.append(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for volume in volumes:\n",
    "    doc = reader.PDFReader(volume).read()\n",
    "    volume_name = volume.name.removesuffix(\".pdf\").replace(\".\", \"_\")\n",
    "    transcriptions = [\n",
    "        transcription\n",
    "        for transcription in master_transcriptions\n",
    "        if transcription.name.startswith(volume_name)\n",
    "    ]\n",
    "\n",
    "    for index, transcription in enumerate(transcriptions):\n",
    "        page_number = int(\n",
    "            txt_pdf_conversion_table[volume_name][\n",
    "                transcription.name.removesuffix(\".txt\")\n",
    "            ]\n",
    "        )\n",
    "        page_text = get_page_text_from_pdf(\n",
    "            doc,\n",
    "            page_number,\n",
    "        )\n",
    "        master_text = reader.TextReader(Path(str(transcription.absolute()).replace(\".txt\", \"_classified.txt\"))).read()\n",
    "        training_set = data.create_openai_dataset(\n",
    "            system_role=messages.SYSTEM_ROLE,\n",
    "            user_template=messages.USER_TEXT_CLASSIFICATION,\n",
    "            user_text=page_text,\n",
    "            assistant_template=messages.ASSISTANT_TEXT_CLASSIFIED,\n",
    "            assistant_text=master_text,\n",
    "        )\n",
    "\n",
    "        if index in validation_indicies:\n",
    "            classification_validation_data.append(training_set)\n",
    "        else:\n",
    "            training_data.append(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training data as a jsonl file and each validation set\n",
    "writer.JSONLWriter(\n",
    "    PROJECT_ROOT / \"data/ZG/openai_training_data.jsonl\"\n",
    ").write(content=[x.to_dict() for x in training_data])\n",
    "\n",
    "writer.JSONLWriter(\n",
    "    PROJECT_ROOT / \"data/ZG/openai_ocr_validation.jsonl\"\n",
    ").write(content=[x.to_dict() for x in ocr_validation_data])\n",
    "\n",
    "writer.JSONLWriter(\n",
    "    PROJECT_ROOT / \"data/ZG/openai_splitting_validation.jsonl\"\n",
    ").write(content=[x.to_dict() for x in splitting_validation_data])\n",
    "\n",
    "writer.JSONLWriter(\n",
    "    PROJECT_ROOT / \"data/ZG/openai_classification_validation.jsonl\"\n",
    ").write(content=[x.to_dict() for x in classification_validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "jsonl_training_data = reader.JSONLReader(\n",
    "    PROJECT_ROOT / \"data/ZG/openai_training_data.jsonl\"\n",
    ").read()\n",
    "\n",
    "print(len(jsonl_training_data) == len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from ssrq_retro_lab.train.validation import validate_openai_training_data\n",
    "\n",
    "assert validate_openai_training_data(jsonl_training_data) is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 1228, 2124\n",
      "mean / median: 1724.9290780141844, 1724.0\n",
      "p5 / p95: 1465.0, 1993.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 570, 1011\n",
      "mean / median: 789.3475177304964, 778.0\n",
      "p5 / p95: 660.0, 931.0\n",
      "\n",
      "0 examples may be over the 16385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from ssrq_retro_lab.train.stats import num_assistant_tokens_from_messages, num_tokens_from_messages,print_distribution\n",
    "\n",
    "TOKEN_LIMIT = 16385\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in jsonl_training_data:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > TOKEN_LIMIT for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the {TOKEN_LIMIT} token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~243215 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~729645 tokens\n",
      "Estimated cost for fine-tuning: approximately $5.84\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(jsonl_training_data)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(TOKEN_LIMIT, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
