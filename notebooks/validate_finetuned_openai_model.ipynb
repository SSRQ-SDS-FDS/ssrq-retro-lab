{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrq_retro_lab.config import PROJECT_ROOT\n",
    "from ssrq_retro_lab.repository import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = (PROJECT_ROOT / \"data/ZG\")\n",
    "validation_files =[\"openai_classification_validation.jsonl\", \"openai_ocr_validation.jsonl\", \"openai_splitting_validation.jsonl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrq_retro_lab.train import train\n",
    "\n",
    "model_id = train.get_finetuned_model_id(\"ftjob-aRl2LY061gfRiEowCpHNyphM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def filter_assistant_from_message(messages: dict[str, list[dict[str, str]]]) -> list[dict[str, str]]:\n",
    "    return [item for item in messages['messages'] if item['role'] != 'assistant']\n",
    "\n",
    "def model_completion(model_id: str, messages: dict[str, list[dict[str, str]]]):\n",
    "    res = openai.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=filter_assistant_from_message(messages=messages), # type: ignore\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        seed=5\n",
    "    )\n",
    "    return res\n",
    "\n",
    "def extract_text_from_completion(completion: str, split_index: int = 1) -> str:\n",
    "    if \"```\" in completion:\n",
    "        return completion.split(\"```\")[split_index]\n",
    "\n",
    "    raise ValueError(\"Completion does not contain ``` – cannot extract text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_validation_data = reader.JSONLReader((training_path / validation_files[1])).read()\n",
    "\n",
    "preditions: list[str] = []\n",
    "actuals: list[str] = []\n",
    "\n",
    "for item in ocr_validation_data:\n",
    "    expected_text = extract_text_from_completion([x for x in item['messages'] if x['role'] == 'assistant'][0]['content'])\n",
    "    preditions.append(expected_text)\n",
    "    actual_completion = model_completion(model_id=model_id, messages=item)\n",
    "    actual_text = actual_completion.choices[0].message.content\n",
    "    assert actual_text is not None\n",
    "    actuals.append(extract_text_from_completion(actual_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will validate the quality finetuned model. We will use the same metrics as in the previous notebook (see [extract pdf text](./extract_pdf_text.ipynb)) as well a some additional metrics typically used in machine learning.\n",
    "\n",
    "The metrics are:\n",
    "- **CER**: Character Error Rate (lower is better)\n",
    "- **Cosine Similarity**: Cosine similarity between the vectorized text and the vectorized ground truth (higher is better)\n",
    "- **Accuracy**: Accuracy of the model; calculated by the following formula: (Total number of correct predictions) / (Total number of predictions) --> (higher is better)\n",
    "- **F-Score**: F-Score of the model – which means the harmonic mean of precision and recall; calculated by the following formula: 2 * (precision * recall) / (precision + recall) --> (higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssrq_retro_lab.validate import ocr as ocr_validation\n",
    "from collections import namedtuple\n",
    "\n",
    "OCRValidation = namedtuple(\n",
    "    \"OCRValidation\",\n",
    "    [\"input_text\", \"expected_text\", \"output_text\", \"cer\", \"cosine_similarity\"],\n",
    ")\n",
    "\n",
    "ocr_validation_results: list[OCRValidation] = []\n",
    "\n",
    "for index, pairs in enumerate(zip(preditions, actuals)):\n",
    "    input_text = extract_text_from_completion(\n",
    "        [x for x in ocr_validation_data[index][\"messages\"] if x[\"role\"] == \"user\"][0][\n",
    "            \"content\"\n",
    "        ],\n",
    "        2,\n",
    "    )\n",
    "    ocr_quality =ocr_validation.calc_error_rate(pairs[0], pairs[1])\n",
    "    ocr_validation_results.append(OCRValidation(input_text, pairs[0], pairs[1], *ocr_quality))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ocr_validation_df = pd.DataFrame(ocr_validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected_text</th>\n",
       "      <th>output_text</th>\n",
       "      <th>cer</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nE. Staatswesen • 572-583\\n377\\nStAZürich A 2...</td>\n",
       "      <td>\\nE. Staatswesen : 572-183 377\\nStAZürich A 25...</td>\n",
       "      <td>\\nE. Staatswesen · 572-583 377\\nStAZürich A 25...</td>\n",
       "      <td>0.866021</td>\n",
       "      <td>0.993117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nD. Die Stadt- und Amtbücher • 462-464\\n345\\n...</td>\n",
       "      <td>\\nD. Die Stadt- und Amtbücher · 462–464 345\\na...</td>\n",
       "      <td>\\nD. Die Stadt- und Amtbücher · 462–464 345\\na...</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.999538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nA. Die geistlichen Grundherrschaften im Mitt...</td>\n",
       "      <td>\\nA. Die geistlichen Grundherrschaften im Mitt...</td>\n",
       "      <td>\\nA. Die geistlichen Grundherrschaften im Mitt...</td>\n",
       "      <td>1.454177</td>\n",
       "      <td>0.985244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n882 \\nIII. Stadt Zug und ihre Vogteien\\nwere...</td>\n",
       "      <td>\\n882 III. Stadt Zug und ihre Vogteien\\nwere u...</td>\n",
       "      <td>\\n882 III. Stadt Zug und ihre Vogteien\\nwere u...</td>\n",
       "      <td>0.846660</td>\n",
       "      <td>0.991894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n976 \\nIV. Die drei Gemeinden des äußeren Amt...</td>\n",
       "      <td>\\n976 IV. Die drei Gemeinden des äußeren Amtes...</td>\n",
       "      <td>\\n976 IV. Die drei Gemeinden des äußeren Amtes...</td>\n",
       "      <td>0.553761</td>\n",
       "      <td>0.995396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nB. Die städtischen Vogteien • 1587-1589\\n877...</td>\n",
       "      <td>\\nB. Die städtischen Vogteien · 1587–1589 877\\...</td>\n",
       "      <td>\\nB. Die städtischen Vogteien · 1587–1589 877\\...</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.997863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  \\nE. Staatswesen • 572-583\\n377\\nStAZürich A 2...   \n",
       "1  \\nD. Die Stadt- und Amtbücher • 462-464\\n345\\n...   \n",
       "2  \\nA. Die geistlichen Grundherrschaften im Mitt...   \n",
       "3  \\n882 \\nIII. Stadt Zug und ihre Vogteien\\nwere...   \n",
       "4  \\n976 \\nIV. Die drei Gemeinden des äußeren Amt...   \n",
       "5  \\nB. Die städtischen Vogteien • 1587-1589\\n877...   \n",
       "\n",
       "                                       expected_text  \\\n",
       "0  \\nE. Staatswesen : 572-183 377\\nStAZürich A 25...   \n",
       "1  \\nD. Die Stadt- und Amtbücher · 462–464 345\\na...   \n",
       "2  \\nA. Die geistlichen Grundherrschaften im Mitt...   \n",
       "3  \\n882 III. Stadt Zug und ihre Vogteien\\nwere u...   \n",
       "4  \\n976 IV. Die drei Gemeinden des äußeren Amtes...   \n",
       "5  \\nB. Die städtischen Vogteien · 1587–1589 877\\...   \n",
       "\n",
       "                                         output_text       cer  \\\n",
       "0  \\nE. Staatswesen · 572-583 377\\nStAZürich A 25...  0.866021   \n",
       "1  \\nD. Die Stadt- und Amtbücher · 462–464 345\\na...  0.252525   \n",
       "2  \\nA. Die geistlichen Grundherrschaften im Mitt...  1.454177   \n",
       "3  \\n882 III. Stadt Zug und ihre Vogteien\\nwere u...  0.846660   \n",
       "4  \\n976 IV. Die drei Gemeinden des äußeren Amtes...  0.553761   \n",
       "5  \\nB. Die städtischen Vogteien · 1587–1589 877\\...  0.188235   \n",
       "\n",
       "   cosine_similarity  \n",
       "0           0.993117  \n",
       "1           0.999538  \n",
       "2           0.985244  \n",
       "3           0.991894  \n",
       "4           0.995396  \n",
       "5           0.997863  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_validation_df.to_pickle('./pkl_cache/openai_ocr_correction_validation_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average CER is 0.6935633046155497\n",
      "The average cosine similarity is 0.9938420018986515\n"
     ]
    }
   ],
   "source": [
    "print(f\"The average CER is {ocr_validation_df['cer'].mean()}\")\n",
    "print(f\"The average cosine similarity is {ocr_validation_df['cosine_similarity'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_start_marker(text: str) -> list[str]:\n",
    "    return re.findall(r'\\[START\\]', text)\n",
    "\n",
    "splitting_validation_data = reader.JSONLReader((training_path / validation_files[2])).read()\n",
    "\n",
    "expected_splitted_texts: list[str] = []\n",
    "expected_starts: list[str] = []\n",
    "actual_splitted_texts: list[str] = []\n",
    "actual_starts: list[str] = []\n",
    "\n",
    "for item in splitting_validation_data:\n",
    "    expected_text = [x for x in item['messages'] if x['role'] == 'assistant'][0]['content']\n",
    "    expected_splitted_texts.append(expected_text)\n",
    "    expected_starts.extend(extract_start_marker(expected_text))\n",
    "    actual_completion = model_completion(model_id=model_id, messages=item)\n",
    "    actual_text = actual_completion.choices[0].message.content\n",
    "    assert actual_text is not None\n",
    "    actual_splitted_texts.append(actual_text)\n",
    "    actual_starts.extend(extract_start_marker(actual_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_splitted_texts</th>\n",
       "      <th>actual_splitted_text</th>\n",
       "      <th>expected_start_pos</th>\n",
       "      <th>actual_start_pos</th>\n",
       "      <th>expected_start_markers</th>\n",
       "      <th>actual_start_markers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[START]StAZürich A 259.4.-1767 Juni 1. Persona...</td>\n",
       "      <td>[START]StAZürich A 259.4. - 1767 Juni 1. Perso...</td>\n",
       "      <td>[0, 222, 533, 704, 1556]</td>\n",
       "      <td>[0, 224, 535, 1578]</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[START]altgeübtem brauch diese worte hinzuzuse...</td>\n",
       "      <td>[START]altgeübtem brauch diese worte hinzuzuse...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[START]- 1340-1341 März 30/22. gebietet Herzog...</td>\n",
       "      <td>[START]-1340-1341 März 30/22. gebietet Herzog ...</td>\n",
       "      <td>[0, 1546]</td>\n",
       "      <td>[0, 1546]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[START]were und sich daß erfindt undt kundtlic...</td>\n",
       "      <td>[START]were und sich daß erfindt undt kundtlic...</td>\n",
       "      <td>[0, 187, 967]</td>\n",
       "      <td>[0, 187, 962]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[START]w) 1665 NovemberHüraths articul.Auf a. ...</td>\n",
       "      <td>[START]w) 1665 NovemberHüraths articul.Auf a. ...</td>\n",
       "      <td>[0, 499]</td>\n",
       "      <td>[0, 495]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[START][8] Item wer einen härdfellig machet, d...</td>\n",
       "      <td>[START]B. Die städtischen Vogteien · 1587–1589...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0, 1351]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             expected_splitted_texts  \\\n",
       "0  [START]StAZürich A 259.4.-1767 Juni 1. Persona...   \n",
       "1  [START]altgeübtem brauch diese worte hinzuzuse...   \n",
       "2  [START]- 1340-1341 März 30/22. gebietet Herzog...   \n",
       "3  [START]were und sich daß erfindt undt kundtlic...   \n",
       "4  [START]w) 1665 NovemberHüraths articul.Auf a. ...   \n",
       "5  [START][8] Item wer einen härdfellig machet, d...   \n",
       "\n",
       "                                actual_splitted_text  \\\n",
       "0  [START]StAZürich A 259.4. - 1767 Juni 1. Perso...   \n",
       "1  [START]altgeübtem brauch diese worte hinzuzuse...   \n",
       "2  [START]-1340-1341 März 30/22. gebietet Herzog ...   \n",
       "3  [START]were und sich daß erfindt undt kundtlic...   \n",
       "4  [START]w) 1665 NovemberHüraths articul.Auf a. ...   \n",
       "5  [START]B. Die städtischen Vogteien · 1587–1589...   \n",
       "\n",
       "         expected_start_pos     actual_start_pos  expected_start_markers  \\\n",
       "0  [0, 222, 533, 704, 1556]  [0, 224, 535, 1578]                       5   \n",
       "1                       [0]                  [0]                       1   \n",
       "2                 [0, 1546]            [0, 1546]                       2   \n",
       "3             [0, 187, 967]        [0, 187, 962]                       3   \n",
       "4                  [0, 499]             [0, 495]                       2   \n",
       "5                       [0]            [0, 1351]                       1   \n",
       "\n",
       "   actual_start_markers  \n",
       "0                     4  \n",
       "1                     1  \n",
       "2                     2  \n",
       "3                     3  \n",
       "4                     2  \n",
       "5                     2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_all_indices(text: str, pattern: str = \"[START]\") -> list[int]:\n",
    "    start = 0\n",
    "    indices: list[int] = []\n",
    "\n",
    "    while start < len(text):\n",
    "        start = text.find(pattern, start)\n",
    "        if start == -1:\n",
    "            break\n",
    "        indices.append(start)\n",
    "        start += len(pattern)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "SplittingValidation = namedtuple(\n",
    "    \"SplittingValidation\",\n",
    "    [\n",
    "        \"expected_splitted_texts\",\n",
    "        \"actual_splitted_text\",\n",
    "        \"expected_start_pos\",\n",
    "        \"actual_start_pos\",\n",
    "        \"expected_start_markers\",\n",
    "        \"actual_start_markers\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "splitting_validation_results: list[SplittingValidation] = []\n",
    "\n",
    "for pair in zip(expected_splitted_texts, actual_splitted_texts):\n",
    "    cleaned_expected_text = (\n",
    "        extract_text_from_completion(pair[0], 1).replace(\"\\n\", \"\").strip()\n",
    "    )\n",
    "    cleaned_actual_text = (\n",
    "        extract_text_from_completion(pair[1], 1).replace(\"\\n\", \"\").strip()\n",
    "    )\n",
    "    expected_start_pos = find_all_indices(cleaned_expected_text)\n",
    "    actual_start_pos = find_all_indices(cleaned_actual_text)\n",
    "    splitting_validation_results.append(\n",
    "        SplittingValidation(\n",
    "            cleaned_expected_text,\n",
    "            cleaned_actual_text,\n",
    "            expected_start_pos,\n",
    "            actual_start_pos,\n",
    "            len(extract_start_marker(cleaned_expected_text)),\n",
    "            len(extract_start_marker(cleaned_actual_text)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "splitting_validation_df = pd.DataFrame(splitting_validation_results)\n",
    "splitting_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_validation_df.to_pickle('./pkl_cache/openai_splitting_validation_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the splitting is 1.0\n",
      "The F1 score of the splitting is 1.0\n"
     ]
    }
   ],
   "source": [
    "from ssrq_retro_lab.validate import general as general_validation\n",
    "\n",
    "splitting_validation_metrics = general_validation.calc_ml_metrics(expected_starts, actual_starts)\n",
    "\n",
    "print(f\"The accuracy of the splitting is {splitting_validation_metrics.accuracy}\")\n",
    "print(f\"The F1 score of the splitting is {splitting_validation_metrics.f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell just validated the quality of the model on the expected `START`-markers. The following will compare the position of the `START`-markers in the ground truth and the predicted text (the one created by GPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the splitting, measured by index, is 0.14285714285714285\n",
      "The F1 score of the splitting, measured by index, is 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "splitting_validation_df.expected_start_markers\n",
    "\n",
    "splitting_metrics = general_validation.calc_ml_metrics(\n",
    "    list(chain.from_iterable(splitting_validation_df.expected_start_pos)),\n",
    "    list(chain.from_iterable(splitting_validation_df.actual_start_pos))\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The accuracy of the splitting, measured by index, is {splitting_metrics.accuracy}\"\n",
    ")\n",
    "print(\n",
    "    f\"The F1 score of the splitting, measured by index, is {splitting_metrics.f1_score}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the previous calculations the finetuned model seems to perform very bad. The accuracy and F1 score are very low (0,14) for the splitting task. So we will perform some more validation here and compare the actual results line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 600)\n",
    "\n",
    "lines_df = [pd.DataFrame(columns=[\"expected\", \"actual\", \"cer\"]) for _ in expected_splitted_texts]\n",
    "\n",
    "\n",
    "for index, pair in enumerate(zip(expected_splitted_texts, actual_splitted_texts)):\n",
    "    cleaned_expected_text = extract_text_from_completion(pair[0], 1).strip()\n",
    "    cleaned_actual_text = extract_text_from_completion(pair[1], 1).strip()\n",
    "\n",
    "    lines1 = cleaned_expected_text.splitlines()\n",
    "    lines2 = cleaned_actual_text.splitlines()\n",
    "\n",
    "    for line_index, line in enumerate(zip(lines1, lines2)):\n",
    "        metrics = ocr_validation.calc_error_rate(line[0], line[1])\n",
    "        lines_df[index].loc[line_index] = [line[0], line[1], metrics[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've we take a close at the CER in a line by line comparison the results are overall much better. Furthermore we can see why we have such a low accuracy and F1 score. While the model is very good at predicting the `START`-markers it seems rather bad in ignoring the headers as formulated in the prompt:\n",
    "\n",
    "```txt \n",
    "Take a deep breath and look at the following corrected text, which was created with OCR. The text consists of different sections. A section represents the transcribed text of a document. This section usually starts with a date line and a title. Analyse the text and divide it into the different sections. Mark the start of a section with [START]. Headers can be ignored. Do not make any other changes and retain the line breaks. Note: There may be fragments from the previous page - also mark them as a section. The text starts and ends with ``` as markers. Here is the text: \\n\\n ```\\n[[text]]\\n```\n",
    "```\n",
    "\n",
    "The last entry of the validation set is a good example for this. In our comparison we can see a classical off-by-one error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected</th>\n",
       "      <th>actual</th>\n",
       "      <th>cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[START]</td>\n",
       "      <td>[START]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[8] Item wer einen härdfellig machet, der ist dem vogt undt den</td>\n",
       "      <td>B. Die städtischen Vogteien · 1587–1589 877</td>\n",
       "      <td>79.365079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vieren verfallen neün pfundt undt dem sächer drü pfundt ze bueß,</td>\n",
       "      <td>[8] Item wer einen härdfellig machet, der ist dem vogt undt den</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wann es gekhlagt wirdt.</td>\n",
       "      <td>vieren verfallen neün pfundt undt dem sächer drü pfundt ze bueß,</td>\n",
       "      <td>84.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9] Item wer auch den anderen bluethruß machet, der ist verfallen</td>\n",
       "      <td>wann es gekhlagt wirdt.</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dem vogt und den vieren ze bueß neün pfundt und dem sächer drü</td>\n",
       "      <td>[9] Item wer auch den anderen bluethruß machet, der ist verfallen</td>\n",
       "      <td>70.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pfundt, so es klagt wirdt.</td>\n",
       "      <td>dem vogt und den vieren ze bueß neün pfundt und dem sächer drü</td>\n",
       "      <td>82.258065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[10] Item welcher gegen dem anderen stahel oder isen wirfft, wirdt</td>\n",
       "      <td>pfundt, so es klagt wirdt.</td>\n",
       "      <td>78.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>es klagt, so ist er dem vogt und den vieren fünff pfundt ze bueß</td>\n",
       "      <td>[10] Item welcher gegen dem anderen stahel oder isen wirfft, wirdt</td>\n",
       "      <td>80.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>schuldig.</td>\n",
       "      <td>es klagt, so ist er dem vogt und den vieren fünff pfundt ze bueß</td>\n",
       "      <td>93.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             expected  \\\n",
       "0                                                             [START]   \n",
       "1     [8] Item wer einen härdfellig machet, der ist dem vogt undt den   \n",
       "2    vieren verfallen neün pfundt undt dem sächer drü pfundt ze bueß,   \n",
       "3                                             wann es gekhlagt wirdt.   \n",
       "4   [9] Item wer auch den anderen bluethruß machet, der ist verfallen   \n",
       "5      dem vogt und den vieren ze bueß neün pfundt und dem sächer drü   \n",
       "6                                          pfundt, so es klagt wirdt.   \n",
       "7  [10] Item welcher gegen dem anderen stahel oder isen wirfft, wirdt   \n",
       "8    es klagt, so ist er dem vogt und den vieren fünff pfundt ze bueß   \n",
       "9                                                           schuldig.   \n",
       "\n",
       "                                                               actual  \\\n",
       "0                                                             [START]   \n",
       "1                         B. Die städtischen Vogteien · 1587–1589 877   \n",
       "2     [8] Item wer einen härdfellig machet, der ist dem vogt undt den   \n",
       "3    vieren verfallen neün pfundt undt dem sächer drü pfundt ze bueß,   \n",
       "4                                             wann es gekhlagt wirdt.   \n",
       "5   [9] Item wer auch den anderen bluethruß machet, der ist verfallen   \n",
       "6      dem vogt und den vieren ze bueß neün pfundt und dem sächer drü   \n",
       "7                                          pfundt, so es klagt wirdt.   \n",
       "8  [10] Item welcher gegen dem anderen stahel oder isen wirfft, wirdt   \n",
       "9    es klagt, so ist er dem vogt und den vieren fünff pfundt ze bueß   \n",
       "\n",
       "         cer  \n",
       "0   0.000000  \n",
       "1  79.365079  \n",
       "2  75.000000  \n",
       "3  84.375000  \n",
       "4  80.000000  \n",
       "5  70.769231  \n",
       "6  82.258065  \n",
       "7  78.787879  \n",
       "8  80.303030  \n",
       "9  93.750000  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_df[-1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(text: str) -> list[str]:\n",
    "    return re.findall(r\"\\[(DOCUMENT|SUMMARY|FRAGMENT)\\]\", text)\n",
    "\n",
    "\n",
    "classification_validation_data = reader.JSONLReader(\n",
    "    (training_path / validation_files[0])\n",
    ").read()\n",
    "\n",
    "expected_classified_texts: list[str] = []\n",
    "expected_categories: list[str] = []\n",
    "actual_classified_texts: list[str] = []\n",
    "actual_categories: list[str] = []\n",
    "\n",
    "for item in classification_validation_data:\n",
    "    expected_text = [x for x in item[\"messages\"] if x[\"role\"] == \"assistant\"][0][\n",
    "        \"content\"\n",
    "    ]\n",
    "    expected_classified_texts.append(\n",
    "        extract_text_from_completion(expected_text)\n",
    "    )\n",
    "    actual_completion = model_completion(model_id=model_id, messages=item)\n",
    "    actual_text = actual_completion.choices[0].message.content\n",
    "    assert actual_text is not None\n",
    "    actual_classified_texts.append(\n",
    "        extract_text_from_completion(actual_text)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextCatValidation = namedtuple(\n",
    "    \"TextCatValidation\",\n",
    "    [\n",
    "        \"expected_classified_text\",\n",
    "        \"actual_classified_text\",\n",
    "        \"expected_categories\",\n",
    "        \"actual_categories\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "text_cat_validation: list[TextCatValidation] = []\n",
    "\n",
    "for pair in zip(expected_classified_texts, actual_classified_texts):\n",
    "    expected_categories = extract_categories(pair[0])\n",
    "    actual_categories = extract_categories(pair[1])\n",
    "    text_cat_validation.append(\n",
    "        TextCatValidation(\n",
    "            pair[0],\n",
    "            pair[1],\n",
    "            expected_categories,\n",
    "            actual_categories\n",
    "        )\n",
    "    )\n",
    "\n",
    "text_cat_validation_df =pd.DataFrame(text_cat_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cat_validation_df.to_pickle('./pkl_cache/openai_text_cat_validation_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classification is 0.2857142857142857\n",
      "The F1 score of the classification is 0.28293135435992584\n"
     ]
    }
   ],
   "source": [
    "classification_ml_metrics = general_validation.calc_ml_metrics(\n",
    "    list(chain.from_iterable(text_cat_validation_df.expected_categories)),\n",
    "    list(chain.from_iterable(text_cat_validation_df.actual_categories))\n",
    ")\n",
    "\n",
    "print(f\"The accuracy of the classification is {classification_ml_metrics.accuracy}\")\n",
    "print(f\"The F1 score of the classification is {classification_ml_metrics.f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
